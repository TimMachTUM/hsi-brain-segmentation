{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying bands: red=768.635, green=564.914, blue=468.147\n",
      "Precision: nan, Recall: 0.1095, F1 Score: 0.1112, Dice Score: 0.1112, Accuracy: 0.7894\n",
      "New best score: 0.1112, bands: (506, 226, 93)\n",
      "Trying bands: red=773.001, green=527.808, blue=466.692\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")  # Add parent directory to path\n",
    "\n",
    "from src.dataset.dataset import build_hsi_testloader, get_wavelengths_from_metadata\n",
    "from src.util.segmentation_util import (\n",
    "    build_segmentation_model,\n",
    "    evaluate_model,\n",
    "    load_model,\n",
    ")\n",
    "from src.util.constants import MODELS_DIR\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import gc\n",
    "\n",
    "\n",
    "def get_interval_from_wavelenths(start, end):\n",
    "    wavelength_array = get_wavelengths_from_metadata()\n",
    "    indices = np.where((wavelength_array >= start) & (wavelength_array <= end))[0]\n",
    "    return indices[0], indices[-1]\n",
    "\n",
    "\n",
    "# Define the intervals:\n",
    "red_interval = get_interval_from_wavelenths(600, 1000)\n",
    "green_interval = get_interval_from_wavelenths(500, 600)\n",
    "blue_interval = get_interval_from_wavelenths(400, 500)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "segmentation_model = build_segmentation_model(\n",
    "    encoder=\"timm-regnetx_320\", architecture=\"Linknet\", device=device, in_channels=3\n",
    ")\n",
    "model_path = MODELS_DIR / \"serene-sweep-9.pth\"\n",
    "segmentation_model = load_model(segmentation_model, model_path, device=device)\n",
    "segmentation_model.eval()\n",
    "\n",
    "num_random_samples = 50\n",
    "best_score = -1.0\n",
    "best_bands = None\n",
    "wavelengths = get_wavelengths_from_metadata()\n",
    "for _ in range(num_random_samples):\n",
    "    # Randomly pick one band from each interval\n",
    "    red_band = random.randint(red_interval[0], red_interval[1])\n",
    "    green_band = random.randint(green_interval[0], green_interval[1])\n",
    "    blue_band = random.randint(blue_interval[0], blue_interval[1])\n",
    "\n",
    "    print(\n",
    "        f\"Trying bands: red={wavelengths[red_band]}, green={wavelengths[green_band]}, blue={wavelengths[blue_band]}\"\n",
    "    )\n",
    "    testloader_target = build_hsi_testloader(\n",
    "        batch_size=1,\n",
    "        rgb=True,\n",
    "        rgb_channels=(red_band, green_band, blue_band),\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on these chosen channels\n",
    "    with torch.no_grad():\n",
    "        _, _, _, _, dice_score = evaluate_model(\n",
    "            segmentation_model, testloader_target, device, with_wandb=False\n",
    "        )\n",
    "\n",
    "    # Update best found so far\n",
    "    if dice_score > best_score:\n",
    "        best_score = dice_score\n",
    "        best_bands = (red_band, green_band, blue_band)\n",
    "        print(f\"New best score: {best_score:.4f}, bands: {best_bands}\")\n",
    "\n",
    "    del testloader_target\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Best band combination: {best_bands} with Dice = {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch [0] DiceLoss: 0.7992 | Loss + L1: 0.8002\n",
      "Epoch [0/5] Batch [1] DiceLoss: 0.9589 | Loss + L1: 0.9599\n",
      "Epoch [0/5] Batch [2] DiceLoss: 0.7920 | Loss + L1: 0.7930\n",
      "Epoch [0/5] Batch [3] DiceLoss: 0.8386 | Loss + L1: 0.8396\n",
      "Epoch [0/5] Batch [4] DiceLoss: 0.7204 | Loss + L1: 0.7214\n",
      "Epoch [1/5] Batch [0] DiceLoss: 0.7992 | Loss + L1: 0.8002\n",
      "Epoch [1/5] Batch [1] DiceLoss: 0.9589 | Loss + L1: 0.9599\n",
      "Epoch [1/5] Batch [2] DiceLoss: 0.7920 | Loss + L1: 0.7930\n",
      "Epoch [1/5] Batch [3] DiceLoss: 0.8386 | Loss + L1: 0.8396\n",
      "Epoch [1/5] Batch [4] DiceLoss: 0.7204 | Loss + L1: 0.7214\n",
      "Epoch [2/5] Batch [0] DiceLoss: 0.7992 | Loss + L1: 0.8002\n",
      "Epoch [2/5] Batch [1] DiceLoss: 0.9589 | Loss + L1: 0.9599\n",
      "Epoch [2/5] Batch [2] DiceLoss: 0.7920 | Loss + L1: 0.7930\n",
      "Epoch [2/5] Batch [3] DiceLoss: 0.8386 | Loss + L1: 0.8396\n",
      "Epoch [2/5] Batch [4] DiceLoss: 0.7204 | Loss + L1: 0.7214\n",
      "Epoch [3/5] Batch [0] DiceLoss: 0.7992 | Loss + L1: 0.8002\n",
      "Epoch [3/5] Batch [1] DiceLoss: 0.9589 | Loss + L1: 0.9599\n",
      "Epoch [3/5] Batch [2] DiceLoss: 0.7920 | Loss + L1: 0.7930\n",
      "Epoch [3/5] Batch [3] DiceLoss: 0.8386 | Loss + L1: 0.8396\n",
      "Epoch [3/5] Batch [4] DiceLoss: 0.7204 | Loss + L1: 0.7214\n",
      "Epoch [4/5] Batch [0] DiceLoss: 0.7992 | Loss + L1: 0.8002\n",
      "Epoch [4/5] Batch [1] DiceLoss: 0.9589 | Loss + L1: 0.9599\n",
      "Epoch [4/5] Batch [2] DiceLoss: 0.7920 | Loss + L1: 0.7930\n",
      "Epoch [4/5] Batch [3] DiceLoss: 0.8386 | Loss + L1: 0.8396\n",
      "Epoch [4/5] Batch [4] DiceLoss: 0.7204 | Loss + L1: 0.7214\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from src.util.segmentation_util import build_criterion, build_segmentation_model\n",
    "from src.dataset.dataset import build_hsi_testloader\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class ChannelSelectionNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple gating network that outputs gating logits for 826 channels.\n",
    "    For a static best-3 selection (dataset-wide), we do not need per-image gating input. \n",
    "    If you want per-image gating, pass some features into forward().\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels=826):\n",
    "        super().__init__()\n",
    "        # Each channel has a trainable logit parameter\n",
    "        self.logits = nn.Parameter(torch.zeros(num_channels))\n",
    "    \n",
    "    def forward(self):\n",
    "        # We apply softmax or a sparse approach. This is not strictly top-3 differentiable,\n",
    "        # but we can pick top-3 from alpha each iteration.\n",
    "        alpha = F.softmax(self.logits, dim=0)  # shape [826]\n",
    "        return alpha\n",
    "\n",
    "testloader_target = build_hsi_testloader(\n",
    "    batch_size=1,\n",
    ")\n",
    "# Instantiate gating network\n",
    "gating_net = ChannelSelectionNet(num_channels=826).to(device)\n",
    "\n",
    "# Build / load your frozen segmentation model (SMP Linknet, etc.)\n",
    "segmentation_model = build_segmentation_model(\n",
    "    encoder='timm-regnetx_320',\n",
    "    architecture='Linknet',\n",
    "    device=device,\n",
    "    in_channels=3\n",
    ")\n",
    "model_path = MODELS_DIR / \"serene-sweep-9.pth\"\n",
    "segmentation_model = load_model(segmentation_model, model_path, device=device)\n",
    "segmentation_model.eval()\n",
    "for param in segmentation_model.parameters():\n",
    "    param.requires_grad = False  # freeze the segmentation model\n",
    "    \n",
    "dice_loss = build_criterion('Dice')\n",
    "num_epochs = 5\n",
    "l1_lambda = 1e-3 \n",
    "optimizer = optim.Adam(gating_net.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gating_net.train()\n",
    "    \n",
    "    for batch_idx, (hsi_image, mask) in enumerate(testloader_target):\n",
    "        hsi_image = hsi_image.to(device)   # shape [B, 826, H, W]\n",
    "        mask = mask.to(device)            # shape [B, H, W] or [B, 1, H, W] depending on your code\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward gating net\n",
    "        alpha = gating_net()  # shape [826]\n",
    "        \n",
    "        # Pick top-3 channels (hard selection). \n",
    "        # This operation is not fully differentiable, but PyTorch can still route gradients\n",
    "        # to those top-3 logits. Over training, 3 channels will dominate.\n",
    "        topk_vals, topk_idx = torch.topk(alpha, 3)  # shape [3]\n",
    "        \n",
    "        # Extract the selected channels: shape [B, 3, H, W]\n",
    "        selected_channels = hsi_image[:, topk_idx, :, :]\n",
    "        \n",
    "        # Forward pass in the frozen segmentation model \n",
    "        # (we do NOT use torch.no_grad(), because we want gating_net to receive gradient \n",
    "        # from the DiceLoss. The seg model won't update anyway because it's frozen.)\n",
    "        preds = segmentation_model(selected_channels)\n",
    "        \n",
    "        # SMP Dice loss expects (preds, target) \n",
    "        # If your seg model outputs raw logits, \"from_logits=True\" is correct\n",
    "        # Make sure mask is shape [B, 1, H, W] or [B, H, W], consistent with your segmentation model output\n",
    "        loss_dice = dice_loss(preds, mask)\n",
    "        \n",
    "        # Add an L1 penalty to encourage alpha to be sparse\n",
    "        l1_penalty = torch.sum(torch.abs(alpha))\n",
    "        loss = loss_dice + l1_lambda * l1_penalty\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Batch [{batch_idx}] \"\n",
    "                f\"DiceLoss: {loss_dice.item():.4f} | Loss + L1: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, DiceLoss: 0.7105, TotalLoss: 0.7135\n",
      "Epoch: 0, Batch: 1, DiceLoss: 0.8017, TotalLoss: 0.8047\n",
      "Epoch: 0, Batch: 2, DiceLoss: 0.6638, TotalLoss: 0.6668\n",
      "Epoch: 0, Batch: 3, DiceLoss: 0.4322, TotalLoss: 0.4352\n",
      "Epoch: 0, Batch: 4, DiceLoss: 0.5988, TotalLoss: 0.6018\n",
      "Epoch: 1, Batch: 0, DiceLoss: 0.7084, TotalLoss: 0.7114\n",
      "Epoch: 1, Batch: 1, DiceLoss: 0.8007, TotalLoss: 0.8037\n",
      "Epoch: 1, Batch: 2, DiceLoss: 0.6623, TotalLoss: 0.6653\n",
      "Epoch: 1, Batch: 3, DiceLoss: 0.4312, TotalLoss: 0.4342\n",
      "Epoch: 1, Batch: 4, DiceLoss: 0.5954, TotalLoss: 0.5984\n",
      "Epoch: 2, Batch: 0, DiceLoss: 0.7066, TotalLoss: 0.7096\n",
      "Epoch: 2, Batch: 1, DiceLoss: 0.7999, TotalLoss: 0.8029\n",
      "Epoch: 2, Batch: 2, DiceLoss: 0.6609, TotalLoss: 0.6639\n",
      "Epoch: 2, Batch: 3, DiceLoss: 0.4302, TotalLoss: 0.4332\n",
      "Epoch: 2, Batch: 4, DiceLoss: 0.5919, TotalLoss: 0.5949\n",
      "Epoch: 3, Batch: 0, DiceLoss: 0.7048, TotalLoss: 0.7078\n",
      "Epoch: 3, Batch: 1, DiceLoss: 0.7991, TotalLoss: 0.8021\n",
      "Epoch: 3, Batch: 2, DiceLoss: 0.6596, TotalLoss: 0.6626\n",
      "Epoch: 3, Batch: 3, DiceLoss: 0.4294, TotalLoss: 0.4324\n",
      "Epoch: 3, Batch: 4, DiceLoss: 0.5885, TotalLoss: 0.5915\n",
      "Epoch: 4, Batch: 0, DiceLoss: 0.7031, TotalLoss: 0.7061\n",
      "Epoch: 4, Batch: 1, DiceLoss: 0.7984, TotalLoss: 0.8014\n",
      "Epoch: 4, Batch: 2, DiceLoss: 0.6582, TotalLoss: 0.6612\n",
      "Epoch: 4, Batch: 3, DiceLoss: 0.4286, TotalLoss: 0.4316\n",
      "Epoch: 4, Batch: 4, DiceLoss: 0.5851, TotalLoss: 0.5881\n",
      "Epoch: 5, Batch: 0, DiceLoss: 0.7014, TotalLoss: 0.7044\n",
      "Epoch: 5, Batch: 1, DiceLoss: 0.7977, TotalLoss: 0.8007\n",
      "Epoch: 5, Batch: 2, DiceLoss: 0.6567, TotalLoss: 0.6597\n",
      "Epoch: 5, Batch: 3, DiceLoss: 0.4279, TotalLoss: 0.4309\n",
      "Epoch: 5, Batch: 4, DiceLoss: 0.5817, TotalLoss: 0.5847\n",
      "Epoch: 6, Batch: 0, DiceLoss: 0.6997, TotalLoss: 0.7027\n",
      "Epoch: 6, Batch: 1, DiceLoss: 0.7970, TotalLoss: 0.8000\n",
      "Epoch: 6, Batch: 2, DiceLoss: 0.6553, TotalLoss: 0.6583\n",
      "Epoch: 6, Batch: 3, DiceLoss: 0.4274, TotalLoss: 0.4304\n",
      "Epoch: 6, Batch: 4, DiceLoss: 0.5785, TotalLoss: 0.5815\n",
      "Epoch: 7, Batch: 0, DiceLoss: 0.6980, TotalLoss: 0.7010\n",
      "Epoch: 7, Batch: 1, DiceLoss: 0.7963, TotalLoss: 0.7993\n",
      "Epoch: 7, Batch: 2, DiceLoss: 0.6539, TotalLoss: 0.6569\n",
      "Epoch: 7, Batch: 3, DiceLoss: 0.4269, TotalLoss: 0.4299\n",
      "Epoch: 7, Batch: 4, DiceLoss: 0.5753, TotalLoss: 0.5783\n",
      "Epoch: 8, Batch: 0, DiceLoss: 0.6964, TotalLoss: 0.6994\n",
      "Epoch: 8, Batch: 1, DiceLoss: 0.7957, TotalLoss: 0.7987\n",
      "Epoch: 8, Batch: 2, DiceLoss: 0.6525, TotalLoss: 0.6555\n",
      "Epoch: 8, Batch: 3, DiceLoss: 0.4265, TotalLoss: 0.4295\n",
      "Epoch: 8, Batch: 4, DiceLoss: 0.5722, TotalLoss: 0.5752\n",
      "Epoch: 9, Batch: 0, DiceLoss: 0.6947, TotalLoss: 0.6977\n",
      "Epoch: 9, Batch: 1, DiceLoss: 0.7950, TotalLoss: 0.7980\n",
      "Epoch: 9, Batch: 2, DiceLoss: 0.6510, TotalLoss: 0.6540\n",
      "Epoch: 9, Batch: 3, DiceLoss: 0.4261, TotalLoss: 0.4291\n",
      "Epoch: 9, Batch: 4, DiceLoss: 0.5691, TotalLoss: 0.5721\n",
      "Channel for row 0: 5\n",
      "Channel for row 1: 273\n",
      "Channel for row 2: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from src.util.segmentation_util import build_criterion, build_segmentation_model\n",
    "from src.dataset.dataset import build_hsi_testloader\n",
    "\n",
    "\n",
    "class ChannelSelectionNet(nn.Module):\n",
    "    def __init__(self, num_channels=826):\n",
    "        super().__init__()\n",
    "        # We'll have 3 rows, each row is a set of logits for how to combine the 826 channels\n",
    "        self.alpha = nn.Parameter(torch.zeros(3, num_channels))\n",
    "\n",
    "    def forward(self):\n",
    "        # Softmax over dimension=1 for each of the 3 rows\n",
    "        # shape: [3, 826]\n",
    "        alpha_soft = F.softmax(self.alpha, dim=1)\n",
    "        return alpha_soft\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Build gating net\n",
    "gating_net = ChannelSelectionNet(num_channels=826).to(device)\n",
    "\n",
    "# Build and freeze segmentation model\n",
    "segmentation_model = build_segmentation_model(\n",
    "    encoder=\"timm-regnetx_320\", architecture=\"Linknet\", device=device, in_channels=3\n",
    ")\n",
    "model_path = MODELS_DIR / \"serene-sweep-9.pth\"\n",
    "segmentation_model = load_model(segmentation_model, model_path, device=device)\n",
    "segmentation_model.eval()\n",
    "for param in segmentation_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "dice_loss_fn = build_criterion()\n",
    "optimizer = optim.Adam(gating_net.parameters(), lr=1e-2)\n",
    "\n",
    "trainloader = build_hsi_testloader(batch_size=1)\n",
    "\n",
    "num_epochs = 10\n",
    "l1_lambda = 1e-3  # encourages a few weights in each row to dominate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    gating_net.train()\n",
    "    for batch_idx, (hsi_image, mask) in enumerate(trainloader):\n",
    "        hsi_image = hsi_image.to(device)  # [B, 826, H, W]\n",
    "        mask = mask.to(device)  # [B, H, W] or [B,1,H,W]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        alpha_soft = gating_net()  # shape [3, 826]\n",
    "\n",
    "        # Weighted sum of the 826 channels -> 3 channels\n",
    "        # out_3channels has shape [B, 3, H, W]\n",
    "        out_3channels = torch.einsum(\"bchw,rc->brhw\", hsi_image, alpha_soft)\n",
    "\n",
    "        preds = segmentation_model(out_3channels)\n",
    "        loss_dice = dice_loss_fn(preds, mask)\n",
    "\n",
    "        # L1 penalty on alpha\n",
    "        l1_penalty = torch.sum(torch.abs(alpha_soft))\n",
    "        loss = loss_dice + l1_lambda * l1_penalty\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch}, Batch: {batch_idx}, DiceLoss: {loss_dice.item():.4f}, TotalLoss: {loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# After training, gating_net.alpha_soft likely has 3 rows each nearly \"one-hot\" for a channel\n",
    "gating_net.eval()\n",
    "with torch.no_grad():\n",
    "    alpha_soft = gating_net()  # shape [3, 826]\n",
    "\n",
    "# For each of the 3 rows, find the channel with the largest weight\n",
    "for i in range(3):\n",
    "    best_idx = torch.argmax(alpha_soft[i]).item()\n",
    "    print(f\"Channel for row {i}: {best_idx}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
