# HSI Brain Segmentation

A comprehensive deep learning project for brain segmentation using hyperspectral imaging (HSI) data, featuring domain adaptation techniques and multiple neural network architectures.

## ğŸ§  Project Overview

This project implements advanced deep learning techniques for brain segmentation from hyperspectral imaging data. It includes:

- **Hyperspectral Image Processing**: Working with 826-channel hyperspectral brain images
- **Domain Adaptation**: Feature Adaptation and Domain Adversarial (FADA) methods for cross-domain segmentation
- **Multiple Architectures**: Support for U-Net, U-Net++, LinkNet, FPN, PSPNet, DeepLabV3+, and more
- **Dimensionality Reduction**: Autoencoders, Gaussian channel reduction, and convolutional reducers
- **Ensemble Methods**: Model averaging and majority voting techniques
- **Extensive Evaluation**: Comprehensive metrics including Dice score, IoU, precision, recall

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ ğŸ“ src/                           # Source code
â”‚   â”œâ”€â”€ ğŸ“ dataset/                   # Dataset handling
â”‚   â”‚   â””â”€â”€ dataset.py                # Dataset classes and data loaders
â”‚   â”œâ”€â”€ ğŸ“ model/                     # Neural network models
â”‚   â”‚   â”œâ”€â”€ ğŸ“ FADA/                  # Feature Adaptation Domain Adversarial methods
â”‚   â”‚   â”‚   â”œâ”€â”€ classifier.py         # Classification head
â”‚   â”‚   â”‚   â”œâ”€â”€ discriminator.py      # Domain discriminator
â”‚   â”‚   â”‚   â”œâ”€â”€ feature_extractor.py  # Feature extraction modules
â”‚   â”‚   â”‚   â””â”€â”€ segmentation_model.py # FADA segmentation models
â”‚   â”‚   â”œâ”€â”€ ğŸ“ HSI_models/            # Hyperspectral-specific models
â”‚   â”‚   â”‚   â”œâ”€â”€ HSI_Net.py            # HSI neural network architectures
â”‚   â”‚   â”‚   â””â”€â”€ ensemble_model.py     # Ensemble learning methods
â”‚   â”‚   â””â”€â”€ ğŸ“ dimensionality_reduction/ # Channel reduction techniques
â”‚   â”‚       â”œâ”€â”€ autoencoder.py        # Autoencoder-based reduction
â”‚   â”‚       â”œâ”€â”€ gaussian.py           # Gaussian channel reduction
â”‚   â”‚       â”œâ”€â”€ conv_reducer.py       # Convolutional reducers
â”‚   â”‚       â”œâ”€â”€ cycle_GAN.py          # CycleGAN for domain adaptation
â”‚   â”‚       â””â”€â”€ window_reducer.py     # Wavelength windowing
â”‚   â”œâ”€â”€ ğŸ“ training/                  # Training pipelines
â”‚   â”‚   â”œâ”€â”€ domain_adaptation_training.py  # Domain adaptation training
â”‚   â”‚   â””â”€â”€ supervised_domain_adaptation.py # Supervised domain adaptation
â”‚   â””â”€â”€ ğŸ“ util/                      # Utility functions
â”‚       â”œâ”€â”€ constants.py              # Project constants
â”‚       â”œâ”€â”€ segmentation_util.py      # Segmentation utilities
â”‚       â””â”€â”€ visualize_data.py         # Data visualization tools
â”œâ”€â”€ ğŸ“ scripts/                       # Training and sweep scripts
â”‚   â”œâ”€â”€ FIVES_sweep.py                # FIVES dataset hyperparameter sweeps
â”‚   â”œâ”€â”€ sweep_autoencoder.py          # Autoencoder hyperparameter sweeps
â”‚   â”œâ”€â”€ train_sweep_FADA_supervised.py # Supervised FADA sweeps
â”‚   â””â”€â”€ train_sweep_FADA_unsupervised.py # Unsupervised FADA sweeps
â”œâ”€â”€ ï¿½ notebooks/                     # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ brain-segmentation.ipynb     # Brain segmentation experiments
â”‚   â”œâ”€â”€ fundus-segmentation.ipynb    # Fundus segmentation (baseline)
â”‚   â”œâ”€â”€ domain-adaptation.ipynb      # Domain adaptation experiments
â”‚   â”œâ”€â”€ dataset-analysis.ipynb       # Dataset analysis and visualization
â”‚   â””â”€â”€ search-channels.ipynb        # Optimal channel selection
â”œâ”€â”€ ğŸ“ config/                        # Configuration files for experiments
â”œâ”€â”€ ğŸ“ metrics/                       # Custom evaluation metrics
â”œâ”€â”€ ğŸ“ data/                          # Dataset storage
â”œâ”€â”€ ğŸ“ models/                        # Trained model storage
â”œâ”€â”€ ğŸ“‹ requirements.txt               # Python dependencies
â”œâ”€â”€ ğŸ”§ .env.example                   # Environment variables template
â””â”€â”€ ğŸ”§ .env                           # Environment variables (create from .env.example)
```

## ğŸš€ Quick Start

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/TimMachTUM/hsi-brain-segmentation.git
   cd hsi-brain-segmentation
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**
   ```bash
   # Copy the example environment file
   cp .env.example .env
   
   # Edit .env file to set your data paths
   nano .env  # or use your preferred editor
   ```

4. **Configure data directories**
   
   Edit the `.env` file to set the following paths:
   ```bash
   # FIVES dataset paths
   FIVES_DIR=/path/to/your/FIVES/dataset
   FIVES_RANDOM_CROPS_DIR=/path/to/your/FIVES_RANDOM_CROPS/dataset
   
   # HELICoiD HSI Brain Database paths
   HELICOID_DIR=/path/to/your/HELICoiD/HSI_Human_Brain_Database_IEEE_Access
   HELICOID_WITH_LABELS_DIR=/path/to/your/HELICoiD/with_labels
   
   # Model storage path
   MODELS_DIR=/path/to/your/trained/models
   
   # Ring labels for post-processing
   RING_LABELS_DIR=/path/to/your/ring/labels
   ```

### Environment Setup Guide

The project uses environment variables to manage data paths and configuration. Follow these steps to set up your environment:

1. **Copy the environment template**
   ```bash
   cp .env.example .env
   ```

2. **Edit the `.env` file with your specific paths**
   Open the `.env` file and update the following variables:

   ```bash
   # FIVES Dataset Paths
   # Download from: https://figshare.com/articles/dataset/FIVES_A_Fundus_Image_Dataset_for_AI-based_Vessel_Segmentation/19688169
   FIVES_DIR=/absolute/path/to/FIVES/dataset
   FIVES_RANDOM_CROPS_DIR=/absolute/path/to/FIVES_RANDOM_CROPS/dataset

   # HELICoiD HSI Brain Database Paths
   # Main hyperspectral brain database
   HELICOID_DIR=/absolute/path/to/HELICoiD/HSI_Human_Brain_Database_IEEE_Access
   # Version with ground truth labels
   HELICOID_WITH_LABELS_DIR=/absolute/path/to/HELICoiD/with_labels

   # Model Storage
   # Directory where trained models will be saved
   MODELS_DIR=/absolute/path/to/trained/models

   # Ring Labels (optional)
   # For post-processing ring artifact removal
   RING_LABELS_DIR=/absolute/path/to/ring/labels
   ```

3. **Example directory structure**
   
   Your data directories should be organized as follows:
   ```
   FIVES/
   â”œâ”€â”€ train/
   â”‚   â”œâ”€â”€ Original/
   â”‚   â””â”€â”€ GroundTruth/
   â”œâ”€â”€ validation/
   â”‚   â”œâ”€â”€ Original/
   â”‚   â””â”€â”€ GroundTruth/
   â””â”€â”€ test/
       â”œâ”€â”€ Original/
       â””â”€â”€ GroundTruth/

   HELICoiD/HSI_Human_Brain_Database_IEEE_Access/
   â”œâ”€â”€ 004-02/
   â”‚   â”œâ”€â”€ raw.hdr
   â”‚   â”œâ”€â”€ gtMap.hdr
   â”‚   â”œâ”€â”€ image.jpg
   â”‚   â”œâ”€â”€ darkReference.hdr
   â”‚   â””â”€â”€ whiteReference.hdr
   â”œâ”€â”€ 012-02/
   â”‚   â””â”€â”€ ...
   â””â”€â”€ ...


   ```

### Basic Usage

1. **Train a basic segmentation model**
   ```python
   from src.dataset.dataset import build_hsi_dataloader
   from src.util.segmentation_util import build_segmentation_model, model_pipeline
   
   # Load data
   trainloader, validationloader, testloader = build_hsi_dataloader(
       batch_size=8, window=(500, 600)
   )
   
   # Build model
   model = build_segmentation_model(
       encoder="timm-regnetx_320",
       architecture="Unet",
       in_channels=1
   )
   
   # Train
   model_pipeline(model, trainloader, validationloader, testloader)
   ```

2. **Run domain adaptation**
   ```python
   from src.model.FADA.train_FADA_supervised import model_pipeline
   
   # Configure domain adaptation training
   config = {
       "architecture": "Linknet",
       "encoder": "timm-regnetx_320",
       "learning_rate_fea": 0.001,
       "learning_rate_cls": 0.001,
       "learning_rate_dis": 0.001
   }
   
   model_pipeline(trainloader_source, validationloader_source, 
                  testloader_source, trainloader_target, testloader_target, 
                  config, project="domain-adaptation")
   ```


## ğŸ“Š Key Features

### 1. Hyperspectral Image Processing
- **826-channel processing**: Full spectral range from 400-1000nm
- **Wavelength windowing**: Flexible spectral band selection
- **RGB simulation**: Convert HSI to RGB using optimal channel selection
- **Normalization**: Dark/white reference correction

### 2. Domain Adaptation
- **FADA (Feature Adaptation Domain Adversarial)**: State-of-the-art domain adaptation
- **Supervised/Unsupervised**: Both labeled and unlabeled target domain support
- **Cross-domain validation**: Evaluate on different imaging conditions

### 3. Neural Network Architectures
- **U-Net variants**: U-Net, U-Net++, MA-Net
- **Modern architectures**: LinkNet, FPN, PSPNet, DeepLabV3+, PAN
- **Encoders**: ResNet, RegNet, SENet, ResNeXt, and more
- **Custom layers**: Hyperspectral-specific processing layers

### 4. Dimensionality Reduction
- **Autoencoder-based**: Learnable channel reduction
- **Gaussian reduction**: Statistical channel combination
- **Convolutional reducers**: 1x1 convolutions for channel reduction
- **CycleGAN**: Generative domain adaptation

### 5. Advanced Training Features
- **Weights & Biases integration**: Automatic experiment tracking
- **Hyperparameter sweeps**: Automated hyperparameter optimization
- **Custom loss functions**: Dice, BCE, Focal loss, clDice
- **Data augmentation**: Rotation, flipping, affine transformations

## ğŸ¯ Datasets

### Primary Dataset: HELICoiD HSI Brain Database
- **826 spectral channels** (400-1000nm)
- **High spatial resolution** brain tissue images
- **Ground truth segmentation** masks
- **Multiple subjects** and imaging conditions

### Auxiliary Dataset: FIVES
- **Fundus images** for baseline comparison
- **Retinal vessel segmentation** tasks
- **Domain adaptation experiments**

## ğŸ”§ Configuration

The project uses YAML configuration files for different experiments:

```yaml
# Example config file
name: FADA-RGB-Sweep
method: bayes
parameters:
  architecture:
    value: Linknet
  encoder:
    value: timm-regnetx_320
  in_channels:
    value: 3
  learning_rate_fea:
    min: 0.001
    max: 0.01
```

## ğŸ“ˆ Evaluation Metrics

- **Dice Score**: Harmonic mean of precision and recall
- **Precision/Recall**: Pixel-level accuracy metrics
- **clDice**: Connectivity-aware Dice score
- **F1 Score**: Balanced accuracy measure

## ğŸ§ª Experiments

### Key Notebook Examples

1. **`notebooks/brain-segmentation.ipynb`**: Main HSI brain segmentation experiments
2. **`notebooks/domain-adaptation.ipynb`**: Cross-domain adaptation experiments
3. **`notebooks/search-channels.ipynb`**: Optimal spectral channel selection
4. **`notebooks/dataset-analysis.ipynb`**: Data exploration and visualization

### Hyperparameter Sweeps

The project includes automated hyperparameter optimization:

```bash
# FIVES dataset sweep
python scripts/FIVES_sweep.py

# Autoencoder sweep
python scripts/sweep_autoencoder.py

# Domain adaptation sweeps
python scripts/train_sweep_FADA_supervised.py
python scripts/train_sweep_FADA_unsupervised.py
```

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## ğŸ“§ Contact

- **Author**: Tim Mach
- **Institution**: Technical University of Munich (TUM)
- **Project**: Interdisciplinary Project: Physics-Inspired Deep Learning for Hyperspectral Brain Tumor Surgery Imaging
